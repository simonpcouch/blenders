% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/benchmark_model.R
\name{benchmark_model}
\alias{benchmark_model}
\title{Ensemble benchmarking}
\usage{
benchmark_model(workflow_set, data, meta_learner, steps)
}
\arguments{
\item{workflow_set}{An untrained workflow set containing the base learners
for the model stack.}

\item{data}{A dataset to benchmark with---will be split into training and
testing, and resampled, internally.}

\item{meta_learner}{A parsnip model specification giving the model that
will combine the predictions. Should contain specification of
hyperparameters to tune.}

\item{steps}{A tibble of recipes steps to apply to the data stack. The
first column is a step function, and the second is a quosure giving the
selector (likely as a tidyselect helper). This object structure is
needed since the recipe can't be defined until it has a data structure
to prep on---the recipe is defined after the data stack is constructed.}
}
\value{
A list, with elements
\itemize{
\item \code{time_to_fit}: The time to tune over the workflowset (this will change),
construct the data stack, fit the meta-learner, and fit members with
non-zero coefficients.
\item \code{metric}: The metric automatically determined by tune.
\item \code{metric_value}: The value of that metric.
}
}
\description{
A quick and scrappy utility for benchmarking a model stack.
}
