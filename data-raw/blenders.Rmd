# Benchmarking stacks

Source code for benchmarking various meta-learners with a number of different datasets.

```{r}
library(tidymodels)

library(blenders)

library(furrr)

# parsnip extensions
library(rules)
library(bonsai)
library(stacks)

# data sources
library(KingCountyHouses)
library(tidytuesdayR)
library(modeldata)
library(ongoal)
```

# Setup

We start off by defining several model specifications as well as accompanying pre-processing. These model specifications will be used both as base learners (`n_base_learners` times per dataset) and meta-learners (once per dataset). Each specification defines hyperparameters to tune.

```{r}
# define model specs and recipes -----------------------------------------------
spec_lr <-
  linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

rec_lr <-
  tribble(
    ~steps,                 ~selectors,
    step_impute_mode,       quo(all_nominal_predictors()),
    step_impute_mean,       quo(all_numeric_predictors()),
    step_dummy,             quo(all_nominal_predictors()),
    step_zv,                quo(all_predictors()),
    step_corr,              quo(all_numeric_predictors()),
    step_pca,               quo(all_numeric_predictors())
  )

spec_bt <-
  boost_tree(mtry = tune(), min_n = tune()) %>%
  set_engine("lightgbm") %>%
  set_mode("regression")

spec_dt <-
  decision_tree(cost_complexity = tune(), tree_depth = tune()) %>%
  set_engine("rpart") %>%
  set_mode("regression")

rec_dt <-
  rec_lr %>% filter(FALSE)

spec_svm <-
  svm_linear(cost = tune(), margin = tune()) %>%
  set_engine("LiblineaR") %>%
  set_mode("regression")

rec_svm <-
  bind_rows(
    rec_lr,
    tribble(
      ~steps,                 ~selectors,
      step_normalize,         quo(all_numeric_predictors()),
      step_YeoJohnson,        quo(all_numeric_predictors())
    )
  )

spec_nn <-
  mlp(hidden_units = tune(), penalty = tune()) %>%
  set_engine("nnet") %>%
  set_mode("regression")

spec_knn <-
  nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("regression")

spec_mars <-
  mars(num_terms = tune(), prod_degree = tune()) %>%
  set_engine("earth") %>%
  set_mode("regression")

rec_mars <-
  tribble(
    ~steps,                 ~selectors,
    step_impute_mode,       quo(all_nominal_predictors()),
    step_impute_mean,       quo(all_numeric_predictors()),
    step_dummy,             quo(all_nominal_predictors())
  )

spec_b <-
  bart(trees = tune(), prior_terminal_node_coef = tune()) %>%
  set_engine("dbarts") %>%
  set_mode("regression")

test_specs <-
  tribble(
    ~spec,        ~steps, 
    spec_lr,      rec_lr,
    spec_bt,      rec_lr,
    spec_dt,      rec_dt,
    spec_svm,     rec_svm,
    spec_nn,      rec_svm,
    spec_knn,     rec_svm,
    spec_mars,    rec_mars,
    spec_b,       rec_mars
  )
```

## Regression

We also define a number of datasets to be used in benchmarking. We do some basic pre-processing for each, and further preprocessing will take place at hyperparameter tuning (for both base learners and the meta-learner.)

```{r, cache = TRUE} 
# define data sets to be used in testing ---------------------------------------
home_prices <-
  KingCountyHouses::home_prices %>%
  select(-date_sold) %>%
  mutate(across(where(is.character), as.factor))

concrete <- 
   modeldata::concrete %>% 
   group_by(across(-compressive_strength)) %>% 
   summarize(compressive_strength = mean(compressive_strength),
             .groups = "drop")

wind_turbine_raw <- tt_load("2020-10-27")$`wind-turbine`

wind_turbine <-
  wind_turbine_raw %>%
  rename(capacity = total_project_capacity_mw) %>%
  mutate(
    turbine_number_in_project = gsub("[[:digit:]]/", "", turbine_number_in_project),
    turbine_number_in_project = as.numeric(turbine_number_in_project),
    across(where(is.character), as.factor)
  ) %>%
  recipe(capacity ~ ., .) %>%
  step_select(-objectid, -notes, -turbine_identifier, -project_name) %>%
  step_other(manufacturer, model, province_territory) %>%
  step_dummy(all_nominal_predictors()) %>%
  prep() %>%
  bake(NULL)

epa_cars_raw <- tt_load("2019-10-15")$big_epa_cars

epa_cars <- 
  epa_cars_raw %>%
  mutate(mpg = if_else(highway08 != 0, highway08, highwayA08)) %>%
  mutate(across(where(is.character), as.factor)) %>%
  recipe(mpg ~ .) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_rm(contains("City"), contains("Highway"), contains("dOn"), contains("phev")) %>%
  step_filter_missing(all_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = 150) %>%
  step_zv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  prep() %>%
  bake(NULL)


test_data <-
  tribble(
    ~data,                     ~outcome,                ~name,
    concrete,                  "compressive_strength",  "Concrete",
    wind_turbine,              "capacity",              "Wind Turbines",
    sim_regression(1e4),       "outcome",               "Simulated (linear)",
    sim_regression(
      1e4,
      method = "van_der_laan_2007_2"
    ),                         "outcome",               "Simulated (nonlinear)",
    home_prices,               "price",                 "Home Prices",
    epa_cars,                  "mpg",                   "EPA Cars"
  ) %>%
  rowwise() %>%
  mutate(
    split = list(initial_split(data)),
    data_train = list(training(split)),
    data_test = list(testing(split))
  ) %>%
  select(-data, -split) %>%
  ungroup()
```

Match each dataset up with a base-learner, and then unnest:

```{r}
# construct workflows for model fits -------------------------------------------
# make combinations of specifications and datasets 
configs <-
  test_specs %>%
  rowwise() %>%
  mutate(test_data = list(test_data)) %>%
  unnest(test_data) %>%
  ungroup()
```

Re-"nest" the specifications as workflow sets, by dataset, and tune hyperparameters for each:

```{r, cache = TRUE}
create_wf_map <- function(x) {
  library(bonsai)
  
  pre <- 
    x %>%
    mutate(
      data = data_train, .after = spec
    ) %>%
    select(data, steps, outcome)
  
  wf_set <- 
    workflow_set(
      preproc = pmap(pre, preprocess_data),
      models = x[["spec"]],
      cross = FALSE
    ) %>%
    workflow_map(
      fn = "tune_grid",
      resamples = vfold_cv(x[["data_train"]][[1]], v = 10),
      grid = 10,
      control = control_stack_grid()
    )
  
  tibble(
    name = x[["name"]][[1]], 
    wf_set = list(wf_set), 
    data_test = list(x[["data_test"]][[1]])
  )
}

# create trained workflow sets -------------------------------------------------
plan(multisession, workers = 9)

wf_sets <- 
  configs %>% 
  group_split(name) %>%
  future_map(create_wf_map)

save(wf_sets, file = "data-raw/regression-wf-sets.Rda")
```

Make data stacks out of each of the trained workflow sets, one for each dataset:

```{r}
# constructing data stacks -----------------------------------------------------
data_stacks <- 
  wf_sets %>%
  bind_rows() %>%
  rowwise() %>%
  mutate(
    data_stack = list(stacks() %>% add_candidates(wf_set)),
    .after = name
  ) %>%
  ungroup() %>%
  select(-wf_set)

data_stacks
```

We have a data stack and a test set for each dataset. Now, match each data stack up with each base learner, this time to be used as a meta-learner,

```{r}
stack_configs <-
  data_stacks %>%
  mutate(specs = list(test_specs)) %>%
  unnest(specs) %>%
  rowwise() %>%
  mutate(
    meta_rec = list(preprocess_data(data_stack, steps = steps)),
    meta_learner = list(workflow(preproc = meta_rec, spec = spec))
  ) %>%
  select(name, data_stack, meta_learner, data_test) %>%
  ungroup()

stack_configs
```

Finally, blend predictions and benchmark each:

```{r}
plan(multisession, workers = 9)

res_st <- 
  future_pmap(
    stack_configs,
    benchmark_model
  )

regression_benchmarks <-
  res_st %>%
  bind_rows()

save(regression_benchmarks, file = "data-raw/regression-benchmarks.Rda")
usethis::use_data(regression_benchmarks, overwrite = TRUE)
```

## Classification

Altering the model specifications to use the "classification" mode, and switching out `linear_reg()` for `multinom_reg()`:

```{r}
test_specs_class <-
  test_specs %>%
  rowwise() %>%
  filter(!class(spec)[[1]] %in% "linear_reg") %>%
  mutate(
    spec = list(set_mode(spec, "classification"))
  ) %>%
  bind_rows(
    tibble(
      spec = list(logistic_reg(penalty = tune(), mixture = tune()) %>% 
                    set_engine ("glmnet")), 
      steps = list(rec_lr))
  )
```

Defining new datasets for benchmarking:

```{r, cache = TRUE} 
# define data sets to be used in testing ---------------------------------------
cells <-
  modeldata::cells %>%
  select(-case)

on_goal <-
  ongoal::on_goal %>%
  recipe(on_goal ~ .) %>%
  step_date(date_time, features = c("dow", "month", "year")) %>%
  step_holiday(date_time) %>%
  step_rm(date_time) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_ns(angle, deg_free = 10) %>%
  prep() %>%
  bake(NULL)

beans <-
  beans::beans %>%
  mutate(is_dermason = if_else(class == "dermason", "Yes", "No"),
         is_dermason = as.factor(is_dermason)) %>%
  select(-class)

sim_linear <-
  sim_classification(1e4) %>%
  select(-contains("non_"))

sim_nonlinear <-
  sim_classification(1e4, num_linear = 13) %>%
  select(-starts_with("linear_0"))

water_raw <- tt_load("2021-05-04")$water

water <-
  water_raw %>%
  mutate(report_date = mdy(report_date)) %>%
  filter(status_id %in% c("y", "n")) %>% 
  recipe(status_id ~ .) %>%
  step_date(report_date, keep_original_cols = FALSE) %>%
  step_rm(row_id, status, installer, pay) %>%
  step_filter_missing(threshold = .2) %>%
  step_other(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  prep() %>%
  bake(NULL)
  

test_data_class <-
  tribble(
    ~data,                     ~outcome,                ~name,
    cells,                     "class",                 "Cells",
    sim_linear,                "class",                 "Simulated (Linear)",
    sim_nonlinear,             "class",                 "Simulated (Non-linear)",
    on_goal,                   "on_goal",               "On Goal",
    beans,                     "is_dermason",           "Beans",
    water,                     "status_id",             "Water"
  ) %>%
  rowwise() %>%
  mutate(
    split = list(initial_split(data)),
    data_train = list(training(split)),
    data_test = list(testing(split))
  ) %>%
  select(-data, -split) %>%
  ungroup()
```

<!-- repeat the process from before, switching out modes and specs as needed. -->
