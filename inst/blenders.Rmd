```{r}
library(tidymodels)
library(bonsai)
library(blenders)
library(future)
```

```{r}
# define model specs and recipes -----------------------------------------------
spec_lr <-
  linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

rec_lr <-
  tribble(
    ~steps,                 ~selectors,
    step_dummy,             quo(all_nominal_predictors()),
    step_zv,                quo(all_predictors()),
    step_impute_knn,        quo(all_predictors()),
    step_corr,              quo(all_predictors()),
    step_pca,               quo(all_predictors())
  )

spec_bt <-
  boost_tree(mtry = tune(), min_n = tune()) %>%
  set_engine("lightgbm") %>%
  set_mode("regression")

rec_bt <- rec_lr

spec_svm <-
  svm_linear(cost = tune(), margin = tune()) %>%
  set_engine("LiblineaR") %>%
  set_mode("regression")

rec_svm <-
  bind_rows(
    rec_lr,
    tribble(
      ~steps,                 ~selectors,
      step_normalize,         quo(all_numeric_predictors()),
      step_YeoJohnson,        quo(all_numeric_predictors())
    )
  )

spec_nn <-
  mlp(hidden_units = tune(), penalty = tune()) %>%
  set_engine("nnet") %>%
  set_mode("regression")

rec_nn <- rec_svm

spec_knn <-
  nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("regression")

rec_knn <- rec_svm

test_specs <-
  tribble(
    ~spec,        ~steps, 
    spec_lr,      rec_lr,
    spec_bt,      rec_bt,
    spec_svm,     rec_svm,
    spec_nn,      rec_nn,
    spec_knn,     rec_knn
  )
```

```{r} 
# define data sets to be used in testing ---------------------------------------
tree_frogs <- 
  stacks::tree_frogs %>% 
  na.omit() %>%
  mutate(across(where(is.character), as.factor)) %>%
  dplyr::select(-hatched)

concrete <- 
   modeldata::concrete %>% 
   group_by(across(-compressive_strength)) %>% 
   summarize(compressive_strength = mean(compressive_strength),
             .groups = "drop")

test_data <-
  tribble(
    ~data,                     ~outcome,
    concrete,                  "compressive_strength",
    tree_frogs,                "latency",
    sim_regression(1e4),       "outcome",
    ames,                      "Sale_Price"
  )
```

```{r}
# construct workflows for model fits -------------------------------------------
# make combinations of specifications and datasets 
configs <-
  bind_cols(
    bind_rows(test_specs, test_specs, test_specs, test_specs),
    uncount(test_data, weights = rep(nrow(test_specs), nrow(test_data)))
  )

configs
```

```{r}
# benchmark the base learners --------------------------------------------------
plan(multisession, workers = 8)

res <- 
  furrr::future_map2(
    pmap(configs, construct_workflow),
    uncount(test_data, weights = rep(nrow(test_specs), nrow(test_data))) %>% 
      pull(data),
    benchmark_model
  )

res_base <- bind_rows(res) %>% bind_cols(configs) %>% select(-data)

save(res_base, file = "data-raw/res_base.Rda")
```

```{r}
# benchmark the model stacks ---------------------------------------------------
wf_sets <-
  configs %>% 
  group_split(outcome) %>%
  purrr::map(
    ~workflow_set(
      purrr::pmap(.x[2:4], preprocess_data),
      .x[["spec"]],
      cross = FALSE
    )
  )

wf_set_configs <-
  tibble(
    workflow_set = rep(wf_sets, each = nrow(test_specs) + 1),
    data = rep(test_data$data, each = nrow(test_specs) + 1),
    meta_learner = 
      c(rep(test_specs$spec, times = nrow(test_data)), 
        rep(list(NULL), times = nrow(test_data))
      ),
    steps = list(tibble())
  )

res_st <- 
  furrr::future_pmap(
    wf_set_configs,
    benchmark_model
  )
```






